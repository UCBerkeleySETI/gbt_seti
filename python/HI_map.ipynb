{
 "metadata": {
  "name": "HI_map"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "---------------------------------\n\n# Try 2 #\n\n__ CURRENTLY SET UP FOR SPEC-HPC__\n   \n## Plan: ##\n\n- slice out the HI region for each spectrum\n- perform a 3D Gaussian kernel convolution, with the X,Y coordinates\n  set to have the same angular size on the sky, and with the Z size\n  set to be some reasonable width in frequency\n  - do this by calculating the true angular seperation between points\n    and running a 1D Gaussian weighting filter through it.\n  - the end result of the above will be a regular grid in RA, Dec, with spectra\n    smoothed onto that grid.\n  - then see what reasonable smoothing parameter should be used in the spectral dimension\n- see if I can find any HI in there!\n"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "from subprocess import Popen, PIPE\nfrom threading import Thread\nfrom datetime import datetime\nimport pyfits as pf\nimport pymongo as pm\nfrom numbapro import autojit\nfrom glob import glob\nimport re\nfrom astro.coord import ang_sep\nimport gc\nimport os\n\ndef remove_fits( location='/fast_scratch/ishivvers/tmp/'):\n    '''Deletes fits files from location'''\n    o,e = Popen( 'rm {}*.fits'.format(location), shell=True, stdout=PIPE, stderr=PIPE ).communicate()\n\ndef run_raw(infile, output='/fast_scratch/ishivvers/tmp/'):\n    '''\n    Start a subprocess to run and save a whole row worth of .raw files, using \n     the Siemion code.\n    Produces a file Row_??\\.\\d+\\.\\d+\\.fits where ?? is the row number\n    '''\n    s = 'gpuspec -i {} -b /home/siemion/sw/gbt_seti/lib/bandpass_models/fifteen.bin -c 1 -v -V -s {} -f 1032192 > {}gpuspec.log'.format(infile, output, output)\n    # have it print all the output to the terminal, like it should\n    Popen( s, shell=True ).communicate()\n    return\n\ndef get_freqs( fitsfile ):\n    '''\n    Takes in a Siemion-format fits file location, returns\n     an array of frequencies for that file and the table length.\n    '''\n    hdu = pf.open( fitsfile )\n    l = hdu[0].data.shape[-1]\n    fs = hdu[0].header['fcntr'] + hdu[0].header['deltaf']*np.linspace(-l/2,l/2-1,l)\n    hdu.close()\n    return fs, len(hdu)\n    \ndef get_table( fitsfile, itab ):\n    '''\n    Takes in a Siemion-format fits file location and a table number;\n     gives back the data from that row with center-of-band spikes\n     removed and a tuple of the RA and Dec.\n    \n    Note that it seems like we need to open and close the fits file and\n     cannot simply pull all data from it at once; it is just too big.\n     NOTE: hopefully the memmap=True argument fixes this problem!\n    '''\n    hdu = pf.open( fitsfile, memmap=True )\n    d = hdu[itab].data[0,:]\n    coords = (hdu[itab].header['ra'], hdu[itab].header['dec'])\n    hdu.close()\n    # fix up the middle spikes for the relevant table\n    c = len(d)/32/2\n    s = 2*c\n    d[c::s] = (d[c-1::s] + d[c+1::s])/2\n    return d, coords\n\ndef table_iter( fitsfile ):\n    \"\"\"\n    Takes in a Siemion-format fits file location and opens it,\n     providing an iterator for the data tables within it.\n    Each call to *.next() yields a tuple of (ra, dec)\n     and a numpy array of spectra (with central spikes cleaned).\n    Example:\n     ti = table_iter( 'myfile.fits' )\n     for coords, spec in ti:\n         DO SOMETHING\n    \"\"\"\n    hdu = pf.open( fitsfile, memmap=True )\n    for itab in range(len(hdu)):\n        if not itab%500:\n            # need to close the file and run the garbage collector occasionally,\n            #  else we run out of file handlers! (internals of numpy)\n            hdu.close()\n            gc.collect()\n            hdu = pf.open( fitsfile, memmap=True )\n        coords = (hdu[itab].header['ra'], hdu[itab].header['dec'])\n        d = hdu[itab].data[0,:]\n        # clean the center spikes\n        c = len(d)/32/2\n        s = 2*c\n        d[c::s] = (d[c-1::s] + d[c+1::s])/2\n        yield coords, d\n    hdu.close()\n\ndef find_peaks( d ):\n    std = np.std( d )\n    mean = np.mean( d )\n    peaks = np.arange(len(d))[ np.abs(d-mean)>(10*std) ]\n    return peaks        \n\ndef get_Gaussian( x, y ):\n    '''\n    Auto-fits a Gaussian and returns the central wavelength and integrated flux.\n    '''\n    G = fit_gaussian( x, y, plot=False )\n    totflux = G['A'] * np.abs(G['sigma']) * np.pi**0.5\n    return G['mu'], totflux\n    ",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# CALCULATING AND SAVING THE SPECTRA #\n\n\nclient = pm.MongoClient( )\nDB = client.himap\n## Make sure to run \"ensureIndex({coords:\"2dsphere\"})\" in mongo shell on the db.himap collection!\n\n# set the local storage for each machine, and then move the results\n#  together when ready to build the final map\nlocal_storage = '/fast_scratch/ishivvers/storage/'\n# tempdir holds the .fits files\ntempdir = '/fast_scratch/ishivvers/tmp/'\n# have a logfile for the gpu and the cpu\ngpulog = '/export/home/spec-hpc-01/ishivvers/working/gpulog.txt'\ncpulog = '/export/home/spec-hpc-01/ishivvers/working/cpulog.txt'\n\ndef run_guppi( rawfile ):\n    \"\"\"\n    a wrapper around run_raw, for use with get_spectra\n    \"\"\"\n    with open(gpulog,'a') as logfile:\n        logfile.write( '\\n\\n'+'*'*40+'\\n'+str(datetime.now())+'\\n\\n')\n        logfile.write( 'starting %s\\n' %rawfile )\n    \n    if test_row_complete( rawfile ):\n        with open(gpulog,'a') as logfile:\n            logfile.write( '%s has already been run!\\n' %rawfile )\n        return\n    \n    rowstr = re.search('Row_\\d\\d', rawfile).group()\n    donefiles = glob( tempdir + '*.fits')\n    if any( [rowstr in f for f in donefiles] ):\n        with open(gpulog,'a') as logfile:\n            logfile.write( 'using already-created fits file\\n' )\n    else:\n        with open(gpulog,'a') as logfile:\n            logfile.write( 'running raw file\\n' )\n        run_raw( rawfile )\n    # record in database\n    entry = { \"rawfile\" : rawfile }\n    DB.log.update( entry, {\"$set\": entry}, upsert=True )\n\ndef deal_with_spectra( rawfile, freqw, freqc ):\n    \"\"\"\n    for use with get_spectra\n    \"\"\"\n    rowstr = re.search('Row_\\d\\d', rawfile).group()\n    fitsfile = glob( tempdir + '{}*.fits'.format(rowstr) )[0]\n    with open(cpulog,'a') as logfile:\n        logfile.write( '\\n\\n'+'*'*40+'\\n'+str(datetime.now())+'\\n\\n')\n        logfile.write( 'opening and processing output file: %s\\n' %fitsfile )\n    \n    fs, nhdus = get_freqs( fitsfile )\n    \n    mask = (fs>(freqc-freqw))&(fs<(freqc+freqw))\n    with open(cpulog,'a') as logfile:\n        logfile.write( 'running all tables\\n' )\n    \n    # save frequency array\n    freqs_fn = os.path.split(rawfile)[1].strip('.raw') + '_freqs.npy'\n    np.save( local_storage + freqs_fn, fs[mask] )\n    \n    data = table_iter( fitsfile )\n    # go through, save spectra, and insert everything into the database\n    for i, row in enumerate(data):\n        c, d = row\n        with open(cpulog,'a') as logfile:\n            logfile.write( '%d -- ra: %.6f\\n -- dec: %.6f\\n' %(i, c[0],c[1]) )\n        \n        # if we've already run this one, move on\n        res = DB.log.find_one( {'rawfile':rawfile} )\n        if (res.get('tables') != None) and (i in res.get('tables')):\n            with open(cpulog,'a') as logfile:\n                logfile.write( 'already ran table %d\\n' %i )\n            continue\n        \n        # save the spectrum, and then insert a reference to it into the database\n        spec_fn = os.path.split(rawfile)[1].strip('.raw') + '_spec_%d.npy' %i\n        np.save( local_storage + spec_fn, d[mask] )\n        \n        # the mongoDB lat/long definitions are just slightly different\n        #  than RA, Dec, but normal RA,Dec queries map onto the correct\n        #  coordinates (so you only need to worry about it during an insert).\n        if c[0] > 180:\n            newra = c[0]-360.0\n        else:\n            newra = c[0]\n        entry = { \"coords\":{\"type\":\"Point\",\n                            \"coordinates\":[newra, c[1]]},\n                  \"ra\":c[0],\n                  \"dec\":c[1],\n                  \"freqsFile\":freqs_fn,\n                  \"specFile\":spec_fn }\n        DB.himap.insert( entry )\n        # record our success in the DB\n        DB.log.update( {'rawfile':rawfile}, {'$addToSet':{'tables':i}} )\n    # remove file once we're done\n    o,e = Popen( 'rm %s' %fitsfile, shell=True, stdout=PIPE, stderr=PIPE ).communicate()\n    \ndef get_spectra(srchstr='/kepler_data/kepler_disk_*/disk_*/gpu4/*_Row_*.0000.raw',\n                freqw=150.0, freqc=1420.40575177, ngpus=1):\n    \"\"\"\n    Pull out slices of frequency width [freqw, in km/s] centered around [freqc in MHz]\n     from the spectra created from all of the raw files that match\n     [srchstr].\n    Saves result into host of *.npy savefiles, with references all managed by\n     a MongoDB server running on sting\n    This function uses threads to run the GPU and CPU simultaneously. \n    \"\"\"\n    \n    # convert freqw from km/s to MHz\n    freqw = (freqw/3e5)*freqc\n    \n    rawfiles = glob(srchstr)\n    for ir,rawfile in enumerate(rawfiles):\n        # start the gpu calculation!\n        gpu_thread = Thread( target=run_guppi, args=(rawfile,) )\n        gpu_thread.start()\n        gpu_thread.join()\n        \n        # make sure the previous CPU thread has finished before starting another\n        if ir != 0:\n            cpu_thread.join()\n        cpu_thread = Thread( target=deal_with_spectra, args=(rawfile, freqw, freqc) )\n        cpu_thread.start()\n\ndef test_row_complete( firstRaw, ntables_needed=1000 ):\n    \"\"\"\n    Tests whether or not at least most of a row has been run.\n    \"\"\"\n    res = DB.log.find_one( {'rawfile':firstRaw} )\n    if res == None:\n        return False\n    elif res.get('tables') == None:\n        return False\n    elif len(res['tables']) < ntables_needed:\n        return False\n    else:\n        return True\n            \n",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "get_spectra()",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# HANDLING THE MAPS #\n\n@autojit(target='cpu')\ndef ang_sep_gauss_weight(ra, dec, ras, decs, sigma):\n    \"\"\"\n    Calculates and returns the weights applied to values at\n     each of ras, decs to interpolate/smooth with a Gaussian\n     kernel of width sigma onto the gridpoint ra, dec.\n    \"\"\"\n    a = 1./(sigma * (2.*np.pi)**.5)\n    dists = ang_sep( ra, dec, ras, decs )\n    g = a * np.exp( -0.5 * (dists/sigma)**2 )\n    return g\n\n@autojit(target='cpu')\ndef resample_data( w, rmin=None,rmax=None, dmin=None,dmax=None, rn=10, dn=10):\n    \"\"\"\n    Uses true-sky angular distances to resample the spectra\n     onto a regular grid in RA and Dec, smoothing with a Gaussian kernel.\n     ALL SPECTRA MUST BE THE SAME LENGTH.\n    w: Gaussian width (in degrees)\n    rmin,rmax: RA min and max; data limits if not given\n    dmin,dmax: Dec min and max; data limits if not given\n    rn: Size of array in RA dimension\n    dn: Size of array in Dec dimension\n    \"\"\"\n    \n    # build the working arrays\n    if rmin==None:\n        res = DB.himap.aggregate( [ {\"$group\": {\"_id\":0,\n                                                \"rmin\": {\"$min$\":\"$ra\"}\n                                                }\n                                     } ] )\n        rmin = res[\"result\"][0][\"rmin\"]\n    if rmax==None:\n        res = DB.himap.aggregate( [ {\"$group\": {\"_id\":0,\n                                                \"rmax\": {\"$max$\":\"$ra\"}\n                                                }\n                                     } ] )\n        rmax = res[\"result\"][0][\"rmax\"]\n    if dmin==None:\n        res = DB.himap.aggregate( [ {\"$group\": {\"_id\":0,\n                                                \"dmin\": {\"$min$\":\"$dec\"}\n                                                }\n                                     } ] )\n        dmin = res[\"result\"][0][\"dmin\"]\n    if dmax==None:\n        res = DB.himap.aggregate( [ {\"$group\": {\"_id\":0,\n                                                \"dmax\": {\"$max$\":\"$dec\"}\n                                                }\n                                     } ] )\n        dmax = res[\"result\"][0][\"dmax\"]\n    \n    rvec = np.linspace(rmin, rmax, rn)\n    dvec = np.linspace(dmin, dmax, dn)\n    outspecs = np.zeros( (len(rvec), len(dvec), len(specs[0])) )\n    \n    for i,ra in enumerate(rvec):\n        for j,dec in enumerate(dvec):\n            print 'interpolating to %.5f, %.5f' %(ra, dec)\n            # get all spectra within 3*w of this location\n            radius = 3 * np.deg2rad( w )\n            query = { \"coords\" :\n                      { \"$geoWithin\":\n                        { \"$centerSphere\": [ [ra, dec], radius ] }\n                    }}\n            curs = DB.himap.find( query, {\"ra\":1,\"dec\":1,\"spec_fn\":1} )\n            ras, decs, specs = [], [], []\n            for obs in curs:\n                ras.append( obs['ra'] )\n                decs.apppend( obs['dec'] )\n                spec_fn = obs['spec_fn']\n                specs.append( np.load( spec_fn ) )\n            # calculate the Gaussian weights and apply them\n            weights = ang_sep_gauss_weight(ra, dec, ras, decs, w)\n            outspecs[i,j,:] = np.dot( weights, specs )\n    \n    return outspecs\n            \ndef inspect_sampling( w, rmin=None,rmax=None, dmin=None,dmax=None, rn=10, dn=10 ):\n    \"\"\"\n    Same arguments as before, but instead of calculating everything just\n     produces a plot showing the resampling scheme overlain on the data.\n    \"\"\"\n    # build the working arrays\n    if rmin==None:\n        res = DB.himap.aggregate( [ {\"$group\": {\"_id\":0,\n                                                \"rmin\": {\"$min$\":\"$ra\"}\n                                                }\n                                     } ] )\n        rmin = res[\"result\"][0][\"rmin\"]\n    if rmax==None:\n        res = DB.himap.aggregate( [ {\"$group\": {\"_id\":0,\n                                                \"rmax\": {\"$max$\":\"$ra\"}\n                                                }\n                                     } ] )\n        rmax = res[\"result\"][0][\"rmax\"]\n    if dmin==None:\n        res = DB.himap.aggregate( [ {\"$group\": {\"_id\":0,\n                                                \"dmin\": {\"$min$\":\"$dec\"}\n                                                }\n                                     } ] )\n        dmin = res[\"result\"][0][\"dmin\"]\n    if dmax==None:\n        res = DB.himap.aggregate( [ {\"$group\": {\"_id\":0,\n                                                \"dmax\": {\"$max$\":\"$dec\"}\n                                                }\n                                     } ] )\n        dmax = res[\"result\"][0][\"dmax\"]\n    # get all the ras,decs observed\n    curs = DB.himap.find( {}, {\"ra\":1, \"dec\":1} )\n    ras,decs = [],[]\n    for obs in curs:\n        ras.append( obs['ra'] )\n        decs.append( obs['dec'] )\n    \n    plt.scatter( ras, decs, marker='o', c='b', label='data' )\n    \n    rvec = np.linspace(rmin, rmax, rn)\n    dvec = np.linspace(dmin, dmax, dn)\n    RV,DV = np.meshgrid(rvec,dvec)\n    RV = RV.flatten()\n    DV = DV.flatten()\n    plt.scatter( RV, DV, marker='+', c='k', label='grid' )\n    \n    plt.vlines( np.mean(rvec), np.mean(dvec)-w/2, np.mean(dvec)+w/2, lw=2, c='k' )\n    plt.hlines( np.mean(dvec), np.mean(rvec)-w/2, np.mean(rvec)+w/2, lw=2, c='k',\n               label='kernel' )\n    \n    plt.legend(loc='best')\n    plt.show()",
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}